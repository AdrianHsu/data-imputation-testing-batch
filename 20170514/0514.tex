% Based on a script by Stefano Rosatti
% author: Lukasz Kidzinski
% email: lukasz.kidzinski@stanford.edu
% Stanford University
% August 2016

% #########################################

\documentclass{beamer}
\usepackage{textpos}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\newcommand{\bT}{\mathbf{T}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\vsmall}{\vskip 5px}
\newcommand{\vmedium}{\vskip 10px}
\newcommand{\vbig}{\vskip 20px}
\newcommand{\bA}{\mathbf{A}}
\graphicspath{ {img/} }
\usefonttheme{serif}

\usetheme{stanford}
\author[PIN-CHUN, HSU]{ Mingsheng Long$^{12}$, Yue Cao$^1$, Jianmin Wang$^1$, and Michael I. Jordan$^2$ }
\title[Deep Adaptation Networks]{\LARGE Learning Transferable Features with Deep
Adaptation Networks}
\date[May 17, 2017]{International Conference on Machine Learning, 2015}

\begin{document}

\setbeamercolor{itemize item}{fg=red}
\frame{\titlepage}

\begin{frame}[fragile]{Introduction}
\begin{itemize}
\item{Goal: Enhance the transferability of features from task-specific layers}
\item{Proposed a Deep Adaptation Network DAN architecture}
  \begin{itemize}
    \item{General features can generalize well to a novel task; however, for specific features they cannot bridge the domain discrepancy}
  \end{itemize}

\item{Some ways to enhance feature transferability:}
  \begin{itemize}
    \item{By mean-embedding matching, feature transferability can be enhanced substantially}
    \item{Utilizing multi-layer representations across domains in a reproducing kernel Hilbert space}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Main Breakthrough}
\begin{itemize}
\item{\emph{Generalizes deep CNN to the domain adaptation}}
\item{Deep adaptation of multiple task-specific layers, including output}
\item{Optimal adaptation using multiple kernel two-sample matching}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Deep Learning For Domain Adaptation}
\begin{itemize}
\item{None or very weak supervision in the \emph{target} task (new domain)}
  \begin{itemize}
  \item{Target classifier cannot be reliably trained due to over-fitting}
  \item{Fine-tuning is impossible as it requires substantial supervision}
  \end{itemize}
\item{Generalize related supervised source task to the target task}
  \begin{itemize}
  \item{Deep networks can learn transferable features for adaptation}
  \end{itemize}
\item{Hard to find big source task for learning deep features from scratch}
  \begin{itemize}
  \item{Transfer from deep networks pre-trained on unrelated big dataset}
  \item{Transferring features from distant tasks better than random features}
  \end{itemize}
\end{itemize}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{fig1}
    \caption{\emph{Deep Learning for Domain Adaptation Workflow}}
    \label{fig:mesh2}
\end{figure}
\end{frame}

\begin{frame}[fragile]{How Transferable Are Deep Features?}
\begin{itemize}
  \item{Transferability is restricted by (Yosinski et al. 2014; Glorot et al. 2011)}
  \item{Specialization of higher layer neurons to original task (new task â†“)}
  \item{Disentangling of variations in higher layers enlarges task discrepancy}
  \item{Transferability of features decreases while task discrepancy increases}
\end{itemize}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{fig2}
    \caption{Transferability of features decreases while task discrepancy increases}
    \label{fig:mesh2}
\end{figure}
\end{frame}

\begin{frame}[fragile]{Deep Adaptation Network (DAN)}
\begin{block}{Key Observations (AlexNet) (Krizhevsky et al. 2012)}
\begin{itemize}
  \item{Comprised of five convolutional layers $conv1-conv5$ and three fully connected layers $fc6-fc8$}
  \item{Convolutional layers learn general features: safely transferable} 
  \begin{itemize}
  \item{Safely freeze $conv1-conv3$ \& fine tuned $conv4-conv5$}
  \end{itemize}
  \item{Fully-connected layers fit task specificicy: \emph{NOT} safely transferable} 
  \begin{itemize}
  \item{Deeply adapt $fc6-fc8$ using statistically optimal two-sample matching}
  \end{itemize}
\end{itemize}
\end{block}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{fig3}
    \caption{The DAN architecture for learning transferable features }
    \label{fig:mesh2}
\end{figure}
\end{frame}

\begin{frame}[fragile]{Objective Function}

\begin{block}{Main Problems)}
\begin{itemize}
\item{Feature transferability decreases with increasing task discrepancy}
\item{Higher layers are tailored to specific tasks, NOT safely transferable}
\item{Adaptation effect may vanish in back-propagation of deep networks}
\end{itemize}
\end{block}

\begin{block}{Deep Adaptation with Optimal Matching}
\begin{itemize}
\item{Deep adaptation: match distributions in multiple layers includingoutput}
\item{Optimal matching: maximize two-sample test of multiple kernels}
\end{itemize}
\begin{equation}
\min\limits_\Theta\max\limits_\kappa\frac{1}{n_a}\sum\limits_{i=1}^{n_a}J(\theta(x_i^a),y_i^a)+\lambda\sum\limits_{\ell=l_1}^{l_2}d_k^2(D_s^\ell,D_t^\ell)
\end{equation}
$\lambda > 0$ is a penality, $D_*^\ell=\{h_i^{*\ell}\}$ is the $\ell$-th layer hidden representaion
\end{block}

\end{frame}

\end{document}
