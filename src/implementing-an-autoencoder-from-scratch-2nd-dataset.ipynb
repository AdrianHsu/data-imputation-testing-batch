{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler,scale\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_RATE = 1\n",
    "TESTING_RATE = 1 - TRAINING_RATE\n",
    "MISSING_RATE = 0.5\n",
    "QUERY_RATE = 0.2\n",
    "# np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "#X : array of shape [n_samples, 2]\n",
    "# e.g. X[0,:]  is [0.74, 0.46]\n",
    "# X, y = sklearn.datasets.load_files(\"../dat/wine_quality\") # X is 2 x 1 vector for 200 n_samples , y is 0 or 1\n",
    "data = pd.read_csv(\"../dat/wine_quality/winequality-white.csv\",sep=';')\n",
    "# plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(d):\n",
    "    #Add normalization code here if necessary\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "#     d.ix[:,0:-1] = scale(d.ix[:,0:-1])\n",
    "    d.ix[:,0:-1] = min_max_scaler.fit_transform(d.ix[:,0:-1])\n",
    "    #d['quality'] = d['quality'].apply(lambda x:1.0 if x==6 else 0.0)\n",
    "    d['quality'] = d['quality'].apply(lambda x: int(x) -3)\n",
    "    d = d.iloc[np.random.permutation(len(d))]\n",
    "    t = int(len(d) * TRAINING_RATE)\n",
    "    tn_data = d.iloc[0:t,:]\n",
    "    tt_data = d.iloc[t:,:]\n",
    "    \n",
    "    tn_X = tn_data.ix[:,0:-1]\n",
    "    tn_Y = tn_data.ix[:,-1]\n",
    "    tt_X = tt_data.ix[:,0:-1]\n",
    "    tt_Y = tt_data.ix[:,-1]\n",
    "    \n",
    "    \n",
    "    return tn_X,tn_Y, tt_X, tt_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>0.112760</td>\n",
       "      <td>0.052265</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.148255</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.092025</td>\n",
       "      <td>0.338279</td>\n",
       "      <td>0.114983</td>\n",
       "      <td>0.248260</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.225806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.259615</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.210843</td>\n",
       "      <td>0.078221</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.156794</td>\n",
       "      <td>0.436195</td>\n",
       "      <td>0.131290</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.338710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.144231</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.174699</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.056380</td>\n",
       "      <td>0.031359</td>\n",
       "      <td>0.132251</td>\n",
       "      <td>0.097166</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.290323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.253012</td>\n",
       "      <td>0.029141</td>\n",
       "      <td>0.086053</td>\n",
       "      <td>0.111498</td>\n",
       "      <td>0.250580</td>\n",
       "      <td>0.081164</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.430233</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.253012</td>\n",
       "      <td>0.214724</td>\n",
       "      <td>0.103858</td>\n",
       "      <td>0.149826</td>\n",
       "      <td>0.357309</td>\n",
       "      <td>0.254290</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.096774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.068627</td>\n",
       "      <td>0.198795</td>\n",
       "      <td>0.062883</td>\n",
       "      <td>0.148368</td>\n",
       "      <td>0.101045</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.163678</td>\n",
       "      <td>0.645455</td>\n",
       "      <td>0.197674</td>\n",
       "      <td>0.161290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>0.198795</td>\n",
       "      <td>0.145706</td>\n",
       "      <td>0.160237</td>\n",
       "      <td>0.212544</td>\n",
       "      <td>0.287703</td>\n",
       "      <td>0.163871</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>0.177419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>0.052147</td>\n",
       "      <td>0.207715</td>\n",
       "      <td>0.116725</td>\n",
       "      <td>0.480278</td>\n",
       "      <td>0.163678</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.241935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.102410</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.118694</td>\n",
       "      <td>0.017422</td>\n",
       "      <td>0.255220</td>\n",
       "      <td>0.112975</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.274194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.198795</td>\n",
       "      <td>0.046012</td>\n",
       "      <td>0.074184</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.083527</td>\n",
       "      <td>0.083478</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.403226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.180723</td>\n",
       "      <td>0.015337</td>\n",
       "      <td>0.112760</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.250580</td>\n",
       "      <td>0.115481</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.338710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.336538</td>\n",
       "      <td>0.245098</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.094955</td>\n",
       "      <td>0.132404</td>\n",
       "      <td>0.389791</td>\n",
       "      <td>0.111047</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.306452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.240385</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.136503</td>\n",
       "      <td>0.091988</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.236659</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.104651</td>\n",
       "      <td>0.451613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.150602</td>\n",
       "      <td>0.016871</td>\n",
       "      <td>0.086053</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>0.663636</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.387097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0        0.403846          0.156863     0.216867        0.013804   0.112760   \n",
       "1        0.288462          0.215686     0.144578        0.092025   0.338279   \n",
       "2        0.259615          0.127451     0.210843        0.078221   0.100890   \n",
       "3        0.144231          0.127451     0.174699        0.001534   0.056380   \n",
       "4        0.230769          0.294118     0.253012        0.029141   0.086053   \n",
       "5        0.403846          0.137255     0.253012        0.214724   0.103858   \n",
       "6        0.288462          0.068627     0.198795        0.062883   0.148368   \n",
       "7        0.240385          0.107843     0.198795        0.145706   0.160237   \n",
       "8        0.278846          0.176471     0.156627        0.052147   0.207715   \n",
       "9        0.307692          0.264706     0.102410        0.007669   0.118694   \n",
       "10       0.278846          0.186275     0.198795        0.046012   0.074184   \n",
       "11       0.211538          0.313725     0.180723        0.015337   0.112760   \n",
       "12       0.336538          0.245098     0.132530        0.012270   0.094955   \n",
       "13       0.240385          0.196078     0.132530        0.136503   0.091988   \n",
       "14       0.355769          0.176471     0.150602        0.016871   0.086053   \n",
       "\n",
       "    free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
       "0              0.052265              0.278422  0.148255  0.436364   0.372093   \n",
       "1              0.114983              0.248260  0.157895  0.318182   0.302326   \n",
       "2              0.156794              0.436195  0.131290  0.472727   0.325581   \n",
       "3              0.031359              0.132251  0.097166  0.527273   0.209302   \n",
       "4              0.111498              0.250580  0.081164  0.581818   0.430233   \n",
       "5              0.149826              0.357309  0.254290  0.445455   0.546512   \n",
       "6              0.101045              0.252900  0.163678  0.645455   0.197674   \n",
       "7              0.212544              0.287703  0.163871  0.127273   0.220930   \n",
       "8              0.116725              0.480278  0.163678  0.536364   0.534884   \n",
       "9              0.017422              0.255220  0.112975  0.372727   0.162791   \n",
       "10             0.024390              0.083527  0.083478  0.327273   0.209302   \n",
       "11             0.097561              0.250580  0.115481  0.409091   0.302326   \n",
       "12             0.132404              0.389791  0.111047  0.381818   0.302326   \n",
       "13             0.097561              0.236659  0.120879  0.300000   0.104651   \n",
       "14             0.094077              0.278422  0.115867  0.663636   0.395349   \n",
       "\n",
       "     alcohol  \n",
       "0   0.322581  \n",
       "1   0.225806  \n",
       "2   0.338710  \n",
       "3   0.290323  \n",
       "4   0.580645  \n",
       "5   0.096774  \n",
       "6   0.161290  \n",
       "7   0.177419  \n",
       "8   0.241935  \n",
       "9   0.274194  \n",
       "10  0.403226  \n",
       "11  0.338710  \n",
       "12  0.306452  \n",
       "13  0.451613  \n",
       "14  0.387097  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tn_X,tn_Y,tt_X,tt_Y = preprocessing(data)\n",
    "X, y,tt_X,tt_Y = preprocessing(data)\n",
    "X = X.reset_index(drop=True)\n",
    "X.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X.ix[0,0]\n",
    "X = X.as_matrix() # change type \"DataFrame\" (of Pandas) to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40384615,  0.15686275,  0.21686747, ...,  0.43636364,\n",
       "         0.37209302,  0.32258065],\n",
       "       [ 0.28846154,  0.21568627,  0.14457831, ...,  0.31818182,\n",
       "         0.30232558,  0.22580645],\n",
       "       [ 0.25961538,  0.12745098,  0.21084337, ...,  0.47272727,\n",
       "         0.3255814 ,  0.33870968],\n",
       "       ..., \n",
       "       [ 0.29807692,  0.20588235,  0.13855422, ...,  0.40909091,\n",
       "         0.25581395,  0.12903226],\n",
       "       [ 0.375     ,  0.19607843,  0.1746988 , ...,  0.4       ,\n",
       "         0.19767442,  0.35483871],\n",
       "       [ 0.28846154,  0.02941176,  0.25301205, ...,  0.41818182,\n",
       "         0.60465116,  0.53225806]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1.dot(W2) + b2 # 200 x 2 -> (200 x 3) * (3 x 2)\n",
    "    a2 = sigmoid(z2) # 200 x 2\n",
    "    return np.argmax(a2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    corect_probs = np.square(a2 - X) * (1/2)\n",
    "    data_loss = np.sum(corect_probs)\n",
    "    return (1.0/num_examples) * data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# If you don't fully understand this function don't worry, it just generates the contour plot below.\n",
    "def plot_decision_boundary(pred_func):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 11)\n"
     ]
    }
   ],
   "source": [
    "num_examples = len(X)\n",
    "nn_input_dim = 11\n",
    "nn_output_dim = 11\n",
    "epsilon = 0.01 #GD update\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):                                        \n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(nn_hdim, num_passes=1000, print_loss=False):\n",
    "    np.random.seed() # X is 200 x 2\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim) # 2 x 3\n",
    "    b1 = np.zeros((1, nn_hdim)) # 1 x 3\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim) # 3 x 2\n",
    "    b2 = np.zeros((1, nn_output_dim)) # 1 x 2\n",
    "    \n",
    "    model = {}\n",
    "    \n",
    "    # Gradient Descent in batch\n",
    "    for i in range(0, num_passes):\n",
    "        \n",
    "        # Forward propagation\n",
    "        z1 = X.dot(W1) + b1 #200 x 3 -> (200 x 2) dot (2 x 3)\n",
    "        a1 = sigmoid(z1) # 200 x 3\n",
    "        z2 = a1.dot(W2) + b2 # 200 x 2 -> (200 x 3) * (3 x 2)\n",
    "        a2 = sigmoid(z2) # 200 x 2\n",
    "        # Backpropagation\n",
    "        delta2 = (a2 * (1 - a2)) * (a2 - X) # 200 x 2 -> (200 x 2) * (200 x 2) \n",
    "        dW2 = (a1.T).dot(delta2) # 3 x 2 -> (3 x 200) dot (200 x 2)\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True) # ? x ?\n",
    "        delta1 = (a1 * (1 - a1)) * (delta2.dot(W2.T)) # 200 x 3 -> (200 x 3) * ((200 x 2) dot (2 x 3))\n",
    "        dW1 = (X.T).dot(delta1) # 2 x 3 -> (2 x 200) dot (200 x 3)\n",
    "        db1 = np.sum(delta1, axis=0) # ? x ?\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        if print_loss and i % 100 == 0:\n",
    "          print( \"Loss after iteration %i: %f\" %(i, calculate_loss(model)) )\n",
    "    return model, a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.280142\n",
      "Loss after iteration 100: 0.113761\n",
      "Loss after iteration 200: 0.110930\n",
      "Loss after iteration 300: 0.131266\n",
      "Loss after iteration 400: 0.086886\n",
      "Loss after iteration 500: 0.086524\n",
      "Loss after iteration 600: 0.075433\n",
      "Loss after iteration 700: 0.088773\n",
      "Loss after iteration 800: 0.080344\n",
      "Loss after iteration 900: 0.061464\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a 3-dimensional hidden layer\n",
    "model, a2 = build_model(3, print_loss=True)\n",
    " \n",
    "# Plot the decision boundary -> no use for autoencoder\n",
    "# plot_decision_boundary(lambda x: predict(model, x))\n",
    "# plt.title(\"Decision Boundary for hidden layer size 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39290564,  0.15858842,  0.21009036, ...,  0.10355898,\n",
       "         0.16047613,  0.08408966],\n",
       "       [ 0.40922848,  0.14038352,  0.21156659, ...,  0.06619653,\n",
       "         0.12866923,  0.01921571],\n",
       "       [ 0.36422099,  0.20140749,  0.21548129, ...,  0.13728056,\n",
       "         0.18539498,  0.07649471],\n",
       "       ..., \n",
       "       [ 0.36927183,  0.19647907,  0.22021987, ...,  0.09081342,\n",
       "         0.15075101,  0.01019198],\n",
       "       [ 0.38158401,  0.13542802,  0.19136123, ...,  0.10613965,\n",
       "         0.1143503 ,  0.0931769 ],\n",
       "       [ 0.37822762,  0.18961921,  0.21409694, ...,  0.16791815,\n",
       "         0.22501925,  0.35492935]])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40384615,  0.15686275,  0.21686747, ...,  0.43636364,\n",
       "         0.37209302,  0.32258065],\n",
       "       [ 0.28846154,  0.21568627,  0.14457831, ...,  0.31818182,\n",
       "         0.30232558,  0.22580645],\n",
       "       [ 0.25961538,  0.12745098,  0.21084337, ...,  0.47272727,\n",
       "         0.3255814 ,  0.33870968],\n",
       "       ..., \n",
       "       [ 0.29807692,  0.20588235,  0.13855422, ...,  0.40909091,\n",
       "         0.25581395,  0.12903226],\n",
       "       [ 0.375     ,  0.19607843,  0.1746988 , ...,  0.4       ,\n",
       "         0.19767442,  0.35483871],\n",
       "       [ 0.28846154,  0.02941176,  0.25301205, ...,  0.41818182,\n",
       "         0.60465116,  0.53225806]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.square(a2 - X) * (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
