{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler,scale\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRAINING_RATE = 0.5\n",
    "TESTING_RATE = 1 - TRAINING_RATE\n",
    "MISSING_RATE = 0.3\n",
    "# QUERY_RATE = 0.2\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../dat/wine_quality/winequality-white.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(d):\n",
    "    #Add normalization code here if necessary\n",
    "#     d.ix[:,0:-1] = scale(d.ix[:,0:-1])\n",
    "    #0-1 scale\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    d.ix[:,0:-1] = min_max_scaler.fit_transform(d.ix[:,0:-1])\n",
    "    d['quality'] = d['quality'].apply(lambda x: int(x) -3)\n",
    "    d['fil'] = d['quality'].apply(lambda x:0 if x==0 or x==5 or x==6 else 1)\n",
    "    d = d[d['fil']==1]\n",
    "    d.drop('fil',axis=1,inplace=True)\n",
    "    d['quality'] = d['quality'].apply(lambda x: 1 if x==3 or x == 4 else 0)\n",
    "    d = d.iloc[np.random.permutation(len(d))]\n",
    "    t = int(len(d) * TRAINING_RATE)\n",
    "    tn_data = d.iloc[0:t,:]\n",
    "    tt_data = d.iloc[t:,:]\n",
    "    \n",
    "    tn_X = tn_data.ix[:,0:-1]\n",
    "    tn_Y = tn_data.ix[:,-1]\n",
    "    tt_X = tt_data.ix[:,0:-1]\n",
    "    tt_Y = tt_data.ix[:,-1]\n",
    "    \n",
    "    return tn_X,tn_Y, tt_X, tt_Y\n",
    "#     return tt_X, tt_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "tn_X,tn_Y,tt_X,tt_Y = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def TestMissingGenerate(tt_data,mr=MISSING_RATE):\n",
    "    missing_entry = []\n",
    "    row , col= range(tt_data.shape[0]),range(tt_data.shape[1])\n",
    "    \n",
    "    while len(missing_entry) < tt_data.shape[0] * tt_data.shape[1] * mr:\n",
    "        r = np.random.choice(row)\n",
    "        c = np.random.choice(col)\n",
    "        \n",
    "        if (r,c) not  in missing_entry:\n",
    "            missing_entry.append((r,c))\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    for me in missing_entry:\n",
    "        tt_data.set_value(me[0],tt_data.columns[me[1]],np.nan)\n",
    "    return tt_data, missing_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tn_X = tn_X.reset_index(drop=True)\n",
    "tn_Y = tn_Y.reset_index(drop=True)\n",
    "tt_X = tt_X.reset_index(drop=True)\n",
    "tt_Y = tt_Y.reset_index(drop=True)\n",
    "tt_X, missing_entry = TestMissingGenerate(tt_X)\n",
    "tt_X = tt_X.fillna(0.0)\n",
    "\n",
    "tt_X = tt_X.values\n",
    "tt_oracle = tt_X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = LogisticRegression()\n",
    "M.fit(tn_X, tn_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_weight = np.abs(M.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.17843705e-01,   4.85111027e+00,   2.66635143e-01,\n",
       "          2.99174069e+00,   1.68657969e-03,   2.20744198e+00,\n",
       "          3.19862471e-01,   1.33929162e-01,   1.44271173e-01,\n",
       "          9.80593742e-01,   5.44122918e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X = pd.read_csv(\"../dat/wine_quality/wine_zeroone.csv\",header=None).values\n",
    "X_mask = tt_X.copy()\n",
    "X_mask[tt_X != 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epoch = 200\n",
    "test_p = 0.1\n",
    "nn_hdim = 4\n",
    "lambda_reg = 0.001\n",
    "epsilon = 1e-4 #learning rate for GD\n",
    "reg_lambda = 0.01\n",
    "nn_input_dim = tt_X.shape[1]\n",
    "nn_output_dim = tt_X.shape[1]\n",
    "L = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_weight_matrix = np.tile(feature_weight,(tt_X.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.17843705e-01,   4.85111027e+00,   2.66635143e-01,\n",
       "          2.99174069e+00,   1.68657969e-03,   2.20744198e+00,\n",
       "          3.19862471e-01,   1.33929162e-01,   1.44271173e-01,\n",
       "          9.80593742e-01,   5.44122918e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def autoRec(epoch, X):\n",
    "    Flag = True\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "    z2 = np.zeros((1, nn_output_dim))\n",
    "    L.clear()\n",
    "    for i in range(0, epoch):\n",
    "    #     if i > 600 and Flag:\n",
    "    #         epsilon *= 0.5\n",
    "    #         Flag = False\n",
    "\n",
    "        # Forward propagation\n",
    "\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        loss = np.sum(np.square(X - z2) * X_mask) #* feature_weight_matrix)\n",
    "        loss += (reg_lambda/2) * (1.0/ X.shape[0]) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "        L.append(loss)\n",
    "        print(\"Loss: \" + str(loss) + \", Round: \" + str(i))\n",
    "\n",
    "        # Backpropagation\n",
    "        delta3 = (z2 - X) * X_mask * feature_weight_matrix\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (a1 * (1 - a1))\n",
    "        dW1 = ((X * feature_weight_matrix).T).dot(delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "    return z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5292.32662229, Round: 0\n",
      "Loss: 3367.45020555, Round: 1\n",
      "Loss: 2300.88026444, Round: 2\n",
      "Loss: 1694.57117841, Round: 3\n",
      "Loss: 1306.11928979, Round: 4\n",
      "Loss: 1073.06086634, Round: 5\n",
      "Loss: 904.218674985, Round: 6\n",
      "Loss: 788.499993793, Round: 7\n",
      "Loss: 696.207425292, Round: 8\n",
      "Loss: 626.217920006, Round: 9\n",
      "Loss: 568.229257126, Round: 10\n",
      "Loss: 521.837103824, Round: 11\n",
      "Loss: 482.95470834, Round: 12\n",
      "Loss: 451.006539842, Round: 13\n",
      "Loss: 424.112096415, Round: 14\n",
      "Loss: 401.689085722, Round: 15\n",
      "Loss: 382.746147295, Round: 16\n",
      "Loss: 366.803111187, Round: 17\n",
      "Loss: 353.279405043, Round: 18\n",
      "Loss: 341.814439602, Round: 19\n",
      "Loss: 332.043279856, Round: 20\n",
      "Loss: 323.706418933, Round: 21\n",
      "Loss: 316.564128054, Round: 22\n",
      "Loss: 310.432969359, Round: 23\n",
      "Loss: 305.151100236, Round: 24\n",
      "Loss: 300.589565162, Round: 25\n",
      "Loss: 296.637111634, Round: 26\n",
      "Loss: 293.20299114, Round: 27\n",
      "Loss: 290.209699478, Round: 28\n",
      "Loss: 287.593081332, Round: 29\n",
      "Loss: 285.298539376, Round: 30\n",
      "Loss: 283.280417766, Round: 31\n",
      "Loss: 281.499863295, Round: 32\n",
      "Loss: 279.924130749, Round: 33\n",
      "Loss: 278.52529945, Round: 34\n",
      "Loss: 277.279683274, Round: 35\n",
      "Loss: 276.167023943, Round: 36\n",
      "Loss: 275.170035151, Round: 37\n",
      "Loss: 274.273877801, Round: 38\n",
      "Loss: 273.465821767, Round: 39\n",
      "Loss: 272.734895577, Round: 40\n",
      "Loss: 272.071640015, Round: 41\n",
      "Loss: 271.467869471, Round: 42\n",
      "Loss: 270.916493745, Round: 43\n",
      "Loss: 270.411352737, Round: 44\n",
      "Loss: 269.94708772, Round: 45\n",
      "Loss: 269.519025168, Round: 46\n",
      "Loss: 269.123083524, Round: 47\n",
      "Loss: 268.755690488, Round: 48\n",
      "Loss: 268.41371515, Round: 49\n",
      "Loss: 268.094408379, Round: 50\n",
      "Loss: 267.795353099, Round: 51\n",
      "Loss: 267.514420823, Round: 52\n",
      "Loss: 267.249734928, Round: 53\n",
      "Loss: 266.999638621, Round: 54\n",
      "Loss: 266.762667586, Round: 55\n",
      "Loss: 266.53752611, Round: 56\n",
      "Loss: 266.323066534, Round: 57\n",
      "Loss: 266.118271284, Round: 58\n",
      "Loss: 265.922237291, Round: 59\n",
      "Loss: 265.734162336, Round: 60\n",
      "Loss: 265.553333136, Round: 61\n",
      "Loss: 265.37911487, Round: 62\n",
      "Loss: 265.210941995, Round: 63\n",
      "Loss: 265.048310148, Round: 64\n",
      "Loss: 264.890769015, Round: 65\n",
      "Loss: 264.737916015, Round: 66\n",
      "Loss: 264.589390731, Round: 67\n",
      "Loss: 264.444869963, Round: 68\n",
      "Loss: 264.304063343, Round: 69\n",
      "Loss: 264.166709441, Round: 70\n",
      "Loss: 264.032572305, Round: 71\n",
      "Loss: 263.901438384, Round: 72\n",
      "Loss: 263.773113789, Round: 73\n",
      "Loss: 263.647421855, Round: 74\n",
      "Loss: 263.524200974, Round: 75\n",
      "Loss: 263.403302664, Round: 76\n",
      "Loss: 263.284589852, Round: 77\n",
      "Loss: 263.167935355, Round: 78\n",
      "Loss: 263.053220521, Round: 79\n",
      "Loss: 262.940334036, Round: 80\n",
      "Loss: 262.829170861, Round: 81\n",
      "Loss: 262.719631302, Round: 82\n",
      "Loss: 262.611620184, Round: 83\n",
      "Loss: 262.505046136, Round: 84\n",
      "Loss: 262.399820957, Round: 85\n",
      "Loss: 262.295859081, Round: 86\n",
      "Loss: 262.193077098, Round: 87\n",
      "Loss: 262.091393357, Round: 88\n",
      "Loss: 261.990727626, Round: 89\n",
      "Loss: 261.891000805, Round: 90\n",
      "Loss: 261.79213469, Round: 91\n",
      "Loss: 261.694051785, Round: 92\n",
      "Loss: 261.596675151, Round: 93\n",
      "Loss: 261.49992829, Round: 94\n",
      "Loss: 261.403735063, Round: 95\n",
      "Loss: 261.30801964, Round: 96\n",
      "Loss: 261.212706468, Round: 97\n",
      "Loss: 261.117720268, Round: 98\n",
      "Loss: 261.022986049, Round: 99\n",
      "Loss: 260.928429137, Round: 100\n",
      "Loss: 260.833975224, Round: 101\n",
      "Loss: 260.739550424, Round: 102\n",
      "Loss: 260.645081342, Round: 103\n",
      "Loss: 260.550495155, Round: 104\n",
      "Loss: 260.455719691, Round: 105\n",
      "Loss: 260.360683525, Round: 106\n",
      "Loss: 260.265316065, Round: 107\n",
      "Loss: 260.169547658, Round: 108\n",
      "Loss: 260.07330968, Round: 109\n",
      "Loss: 259.976534639, Round: 110\n",
      "Loss: 259.87915627, Round: 111\n",
      "Loss: 259.781109633, Round: 112\n",
      "Loss: 259.682331205, Round: 113\n",
      "Loss: 259.582758973, Round: 114\n",
      "Loss: 259.48233252, Round: 115\n",
      "Loss: 259.380993109, Round: 116\n",
      "Loss: 259.278683759, Round: 117\n",
      "Loss: 259.175349323, Round: 118\n",
      "Loss: 259.070936553, Round: 119\n",
      "Loss: 258.965394164, Round: 120\n",
      "Loss: 258.858672889, Round: 121\n",
      "Loss: 258.750725533, Round: 122\n",
      "Loss: 258.641507017, Round: 123\n",
      "Loss: 258.530974415, Round: 124\n",
      "Loss: 258.41908699, Round: 125\n",
      "Loss: 258.305806218, Round: 126\n",
      "Loss: 258.191095812, Round: 127\n",
      "Loss: 258.074921731, Round: 128\n",
      "Loss: 257.957252198, Round: 129\n",
      "Loss: 257.838057695, Round: 130\n",
      "Loss: 257.717310965, Round: 131\n",
      "Loss: 257.594987006, Round: 132\n",
      "Loss: 257.471063055, Round: 133\n",
      "Loss: 257.345518576, Round: 134\n",
      "Loss: 257.218335231, Round: 135\n",
      "Loss: 257.089496863, Round: 136\n",
      "Loss: 256.958989457, Round: 137\n",
      "Loss: 256.826801113, Round: 138\n",
      "Loss: 256.692922003, Round: 139\n",
      "Loss: 256.557344336, Round: 140\n",
      "Loss: 256.420062308, Round: 141\n",
      "Loss: 256.28107206, Round: 142\n",
      "Loss: 256.140371625, Round: 143\n",
      "Loss: 255.997960881, Round: 144\n",
      "Loss: 255.853841493, Round: 145\n",
      "Loss: 255.70801686, Round: 146\n",
      "Loss: 255.56049206, Round: 147\n",
      "Loss: 255.411273786, Round: 148\n",
      "Loss: 255.260370295, Round: 149\n",
      "Loss: 255.107791341, Round: 150\n",
      "Loss: 254.953548119, Round: 151\n",
      "Loss: 254.7976532, Round: 152\n",
      "Loss: 254.640120475, Round: 153\n",
      "Loss: 254.480965087, Round: 154\n",
      "Loss: 254.320203374, Round: 155\n",
      "Loss: 254.15785281, Round: 156\n",
      "Loss: 253.993931936, Round: 157\n",
      "Loss: 253.828460309, Round: 158\n",
      "Loss: 253.661458437, Round: 159\n",
      "Loss: 253.492947721, Round: 160\n",
      "Loss: 253.322950401, Round: 161\n",
      "Loss: 253.151489492, Round: 162\n",
      "Loss: 252.978588735, Round: 163\n",
      "Loss: 252.804272538, Round: 164\n",
      "Loss: 252.628565924, Round: 165\n",
      "Loss: 252.451494481, Round: 166\n",
      "Loss: 252.273084305, Round: 167\n",
      "Loss: 252.093361958, Round: 168\n",
      "Loss: 251.912354415, Round: 169\n",
      "Loss: 251.73008902, Round: 170\n",
      "Loss: 251.546593437, Round: 171\n",
      "Loss: 251.361895613, Round: 172\n",
      "Loss: 251.17602373, Round: 173\n",
      "Loss: 250.989006166, Round: 174\n",
      "Loss: 250.80087146, Round: 175\n",
      "Loss: 250.611648268, Round: 176\n",
      "Loss: 250.421365332, Round: 177\n",
      "Loss: 250.230051443, Round: 178\n",
      "Loss: 250.037735412, Round: 179\n",
      "Loss: 249.844446033, Round: 180\n",
      "Loss: 249.650212058, Round: 181\n",
      "Loss: 249.455062165, Round: 182\n",
      "Loss: 249.259024935, Round: 183\n",
      "Loss: 249.062128823, Round: 184\n",
      "Loss: 248.864402135, Round: 185\n",
      "Loss: 248.665873007, Round: 186\n",
      "Loss: 248.46656938, Round: 187\n",
      "Loss: 248.266518984, Round: 188\n",
      "Loss: 248.065749314, Round: 189\n",
      "Loss: 247.864287619, Round: 190\n",
      "Loss: 247.662160879, Round: 191\n",
      "Loss: 247.459395793, Round: 192\n",
      "Loss: 247.256018766, Round: 193\n",
      "Loss: 247.052055892, Round: 194\n",
      "Loss: 246.847532946, Round: 195\n",
      "Loss: 246.64247537, Round: 196\n",
      "Loss: 246.436908264, Round: 197\n",
      "Loss: 246.230856377, Round: 198\n",
      "Loss: 246.024344099, Round: 199\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "res = autoRec(epoch, tt_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10db9c2b0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nECRJAPx/yiBGOdZqQBwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cfb2a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calRMSE(result, oracle):\n",
    "    return np.sum((result - oracle.values) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_oracle = pd.read_csv(\"../dat/wine_quality/wine_zeroone_oracle.csv\",header=None)\n",
    "calRMSE(res, X_oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "calRMSE(res, X_oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60195828011919961"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(M.predict(tt_X) == tt_Y)/len(tt_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tt_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
