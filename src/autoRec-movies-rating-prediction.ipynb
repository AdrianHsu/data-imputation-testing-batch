{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(size):\n",
    "    ratings = []\n",
    "    if size == \"100k\":\n",
    "        path = os.path.join(\"../dat/rec\", \"ml-100k\", \"u.data\")\n",
    "        print(\"Read movie lens 100k data set\")\n",
    "        f = open(path, \"r\")\n",
    "        while (1):\n",
    "            line = f.readline()\n",
    "            if line == \"\":\n",
    "                break\n",
    "            ratings.append(line.split()[0:-1])\n",
    "        f.close()\n",
    "    ratings = np.array(ratings, dtype = np.float32)\n",
    "    # permute the ratings array\n",
    "    ratings = np.random.permutation(ratings)\n",
    "    print(\"Loading data done\")\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read movie lens 100k data set\n",
      "Loading data done\n"
     ]
    }
   ],
   "source": [
    "ratings = get_data(\"100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 770.,  111.,    5.],\n",
       "       [ 653.,  333.,    5.],\n",
       "       [ 896.,   76.,    3.],\n",
       "       ..., \n",
       "       [ 880.,  243.,    2.],\n",
       "       [ 201.,  191.,    4.],\n",
       "       [ 660.,  315.,    4.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_split_data(data_size, test_p):\n",
    "    # Load data and split into train set, test set randomly.\n",
    "    # data_size is either \"100k\", \"1m\", \"10m\" or \"20m\".\n",
    "    # test_p is a float between 0 - 1 indicating the portion of data hold out as test set\n",
    "    print(\"split data randomly\")\n",
    "    # Load ratings, data is already permuted in get_data\n",
    "    ratings = get_data(data_size)\n",
    "    nb_users = int(np.max(ratings[:, 0]))\n",
    "    nb_movies = int(np.max(ratings[:, 1]))\n",
    "    # split test/train set\n",
    "    test_size = int(len(ratings) * test_p)\n",
    "    test_ratings = ratings[:test_size]\n",
    "    train_ratings = ratings[test_size:]\n",
    "    # train_ratings is converted into a matrix\n",
    "    train_M = np.zeros((nb_movies, nb_users), dtype = np.float32)\n",
    "    for rating in train_ratings:\n",
    "        train_M[int(rating[1]-1), int(rating[0]-1)] = rating[2]\n",
    "    # save test and train data in case more training is needed on this split\n",
    "    np.save(\"../dat/rec/\" + data_size + \"_\" + str(int(test_p * 100))+ \"percent_test.npy\", test_ratings)\n",
    "    np.save(\"../date/rec/\" + data_size + \"_\" + str(int(test_p * 100))+ \"percent_trainM.npy\", train_M)\n",
    "    # test_ratings is numpy array of user id | item id | rating\n",
    "    # train_M is numpy array with nb_movies rows and nb_users columns, missing entries are filled with zero\n",
    "    return test_ratings, train_M, nb_users, nb_movies, len(train_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_RMSE(prediction_M, test_ratings):\n",
    "    RMSE = 0\n",
    "    for rating in test_ratings:\n",
    "        RMSE += (rating[2] - prediction_M[int(rating[1] - 1), int(rating[0] - 1)])**2\n",
    "    RMSE = math.sqrt(RMSE / len(test_ratings))\n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_auto(data_size = \"100k\", nb_epoch = 10, test_p = 0.1, nb_hunits = 10, lambda_reg = 0.001, learningrate = 0.01):\n",
    "    test_ratings, train_M, nb_users, nb_movies, k = load_split_data(data_size, test_p)\n",
    "    prediction_M = np.zeros((nb_movies, nb_users), dtype = np.float32)\n",
    "    RMSE_list = [0] * nb_epoch\n",
    "\n",
    "    # set up theano autoencoder structure and update function\n",
    "    X = T.dvector(\"input\")\n",
    "    X_observed = T.dvector(\"observedIndex\")\n",
    "    update_matrix = T.matrix(\"updateIndex\")\n",
    "    V = theano.shared(np.random.randn(nb_hunits, nb_users), name='V')\n",
    "    miu = theano.shared(np.zeros(nb_hunits), name='miu')\n",
    "    W = theano.shared(np.random.randn(nb_users, nb_hunits), name='W')\n",
    "    b = theano.shared(np.zeros(nb_users), name='b')\n",
    "    z1 = T.nnet.sigmoid(V.dot(X) + miu)\n",
    "    z2 = W.dot(z1) + b\n",
    "    loss_reg = 1.0/nb_movies * lambda_reg/2 * (T.sum(T.sqr(V)) + T.sum(T.sqr(W)))\n",
    "    loss = T.sum(T.sqr((X - z2) * X_observed)) + loss_reg\n",
    "    gV, gmiu, gW, gb = T.grad(loss, [V, miu, W, b])\n",
    "\n",
    "    train = theano.function(\n",
    "          inputs=[X, X_observed, update_matrix],\n",
    "          outputs=[z2],\n",
    "          updates=((V, V - learningrate * gV * update_matrix),(miu, miu - learningrate * gmiu),\n",
    "              (W, W - learningrate * gW * update_matrix.T), (b, b - learningrate * gb * X_observed)))\n",
    "\n",
    "    for j in range(nb_epoch):\n",
    "        print(str(j + 1) + \" epoch\")\n",
    "        for i in np.random.permutation(nb_movies):\n",
    "            Ri = train_M[i, :]\n",
    "            Ri_observed = Ri.copy()\n",
    "            Ri_observed[Ri > 0] = 1\n",
    "            update_m = np.tile(Ri_observed, (nb_hunits, 1))\n",
    "            Ri_predicted = train(Ri, Ri_observed, update_m)\n",
    "            prediction_M[i, :] = np.array(Ri_predicted)\n",
    "        RMSE_list[j] = cal_RMSE(prediction_M, test_ratings)\n",
    "\n",
    "    print(\"training complete\")\n",
    "    return nb_epoch, RMSE_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split data randomly\n",
      "Read movie lens 100k data set\n",
      "Loading data done\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../date/rec/100k_10percent_trainM.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c29779f138ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb_epoth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ee0489ecc0f5>\u001b[0m in \u001b[0;36mtrain_auto\u001b[0;34m(data_size, nb_epoch, test_p, nb_hunits, lambda_reg, learningrate)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_auto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"100k\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_hunits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtest_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_movies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_split_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprediction_M\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_movies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mRMSE_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9f4559f77b0a>\u001b[0m in \u001b[0;36mload_split_data\u001b[0;34m(data_size, test_p)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# save test and train data in case more training is needed on this split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../dat/rec/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_p\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"percent_test.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../date/rec/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_p\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\"percent_trainM.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# test_ratings is numpy array of user id | item id | rating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# train_M is numpy array with nb_movies rows and nb_users columns, missing entries are filled with zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../date/rec/100k_10percent_trainM.npy'"
     ]
    }
   ],
   "source": [
    "nb_epoth, Rlist = train_auto()\n",
    "print(Rlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
